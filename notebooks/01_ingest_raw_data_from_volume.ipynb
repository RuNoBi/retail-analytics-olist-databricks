{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b509840c-6018-4a8b-8b59-1d00a275a038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/Volumes/puttaradol/runtest/olist/\"\n",
    "\n",
    "# 1. Customers\n",
    "df_customers = spark.read.option(\"header\", True).csv(base_path + \"olist_customers_dataset.csv\")\n",
    "df_customers.write.mode(\"overwrite\").saveAsTable(\"puttaradol.runtest.olist_customers\")\n",
    "\n",
    "# 2. Orders\n",
    "df_orders = spark.read.option(\"header\", True).csv(base_path + \"olist_orders_dataset.csv\")\n",
    "df_orders.write.mode(\"overwrite\").saveAsTable(\"puttaradol.runtest.olist_orders\")\n",
    "\n",
    "# 3. Order Items\n",
    "df_order_items = spark.read.option(\"header\", True).csv(base_path + \"olist_order_items_dataset.csv\")\n",
    "df_order_items.write.mode(\"overwrite\").saveAsTable(\"puttaradol.runtest.olist_order_items\")\n",
    "\n",
    "# 4. Payments\n",
    "df_payments = spark.read.option(\"header\", True).csv(base_path + \"olist_order_payments_dataset.csv\")\n",
    "df_payments.write.mode(\"overwrite\").saveAsTable(\"puttaradol.runtest.olist_payments\")\n",
    "\n",
    "# 5. Reviews\n",
    "df_reviews = spark.read.option(\"header\", True).csv(base_path + \"olist_order_reviews_dataset.csv\")\n",
    "df_reviews.write.mode(\"overwrite\").saveAsTable(\"puttaradol.runtest.olist_reviews\")\n",
    "\n",
    "# 6. Products\n",
    "df_products = spark.read.option(\"header\", True).csv(base_path + \"olist_products_dataset.csv\")\n",
    "df_products.write.mode(\"overwrite\").saveAsTable(\"puttaradol.runtest.olist_products\")\n",
    "\n",
    "# 7. Sellers\n",
    "df_sellers = spark.read.option(\"header\", True).csv(base_path + \"olist_sellers_dataset.csv\")\n",
    "df_sellers.write.mode(\"overwrite\").saveAsTable(\"puttaradol.runtest.olist_sellers\")\n",
    "\n",
    "# 8. Geolocation\n",
    "df_geo = spark.read.option(\"header\", True).csv(base_path + \"olist_geolocation_dataset.csv\")\n",
    "df_geo.write.mode(\"overwrite\").saveAsTable(\"puttaradol.runtest.olist_geolocation\")\n",
    "\n",
    "# 9. Product Category Translation\n",
    "df_prodcat = spark.read.option(\"header\", True).csv(base_path + \"product_category_name_translation.csv\")\n",
    "df_prodcat.write.mode(\"overwrite\").saveAsTable(\"puttaradol.runtest.product_category_translation\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6329854910671395,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_raw_data_from_volume",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}